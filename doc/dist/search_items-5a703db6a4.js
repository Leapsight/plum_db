searchNodes=[{"doc":"This module implements a persistent, on-disk hash tree that is used predominately for active anti-entropy exchange in Riak. The tree consists of two parts, a set of unbounded on-disk segments and a fixed size hash tree (that may be on-disk or in-memory) constructed over these segments. A graphical description of this design can be found in: docs/hashtree.md Each segment logically represents an on-disk list of (key, hash) pairs. Whereas the hash tree is represented as a set of levels and buckets, with a fixed width (or fan-out) between levels that determines how many buckets of a child level are grouped together and hashed to represent a bucket at the parent level. Each leaf in the tree corresponds to a hash of one of the on-disk segments. For example, a tree with a width of 4 and 16 segments would look like the following: level buckets 1: [0] 2: [0 1 2 3] 3: [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] With each bucket entry of the form {bucket-id, hash} , eg. {0, binary()} . The hash for each of the entries at level 3 would come from one of the 16 segments, while the hashes for entries at level 1 and 2 are derived from the lower levels. Specifically, the bucket entries in level 2 would come from level 3: 0: hash([ 0 1 2 3]) 1: hash([ 4 5 6 7]) 2: hash([ 8 9 10 11]) 3: hash([12 13 14 15]) And the bucket entries in level 1 would come from level 2: 1: hash([hash([ 0 1 2 3]) hash([ 4 5 6 7]) hash([ 8 9 10 11]) hash([12 13 14 15])]) When a (key, hash) pair is added to the tree, the key is hashed to determine which segment it belongs to and inserted/upserted into the segment. Rather than update the hash tree on every insert, a dirty bit is set to note that a given segment has changed. The hashes are then updated in bulk before performing a tree exchange To update the hash tree, the code iterates over each dirty segment, building a list of (key, hash) pairs. A hash is computed over this list, and the leaf node in the hash tree corresponding to the given segment is updated. After iterating over all dirty segments, and thus updating all leaf nodes, the update then continues to update the tree bottom-up, updating only paths that have changed. As designed, the update requires a single sparse scan over the on-disk segments and a minimal traversal up the hash tree. The heavy-lifting of this module is provided by LevelDB. What is logically viewed as sorted on-disk segments is in reality a range of on-disk (segment, key, hash) values written to LevelDB. Each insert of a (key, hash) pair therefore corresponds to a single LevelDB write (no read necessary). Likewise, the update operation is performed using LevelDB iterators. When used for active anti-entropy in Riak, the hash tree is built once and then updated in real-time as writes occur. A key design goal is to ensure that adding (key, hash) pairs to the tree is non-blocking, even during a tree update or a tree exchange. This is accomplished using LevelDB snapshots. Inserts into the tree always write directly to the active LevelDB instance, however updates and exchanges operate over a snapshot of the tree. In order to improve performance, writes are buffered in memory and sent to LevelDB using a single batch write. Writes are flushed whenever the buffer becomes full, as well as before updating the hashtree. Tree exchange is provided by the compare/4 function. The behavior of this function is determined through a provided function that implements logic to get buckets and segments for a given remote tree, as well as a callback invoked as key differences are determined. This generic interface allows for tree exchange to be implemented in a variety of ways, including directly against to local hash tree instances, over distributed Erlang, or over a custom protocol over a TCP socket. See local_compare/2 and do_remote/1 for examples (-ifdef(TEST) only).","ref":"hashtree.html","title":"hashtree","type":"module"},{"doc":"","ref":"hashtree.html#close/1","title":"hashtree.close/1","type":"function"},{"doc":"","ref":"hashtree.html#compare/4","title":"hashtree.compare/4","type":"function"},{"doc":"","ref":"hashtree.html#delete/2","title":"hashtree.delete/2","type":"function"},{"doc":"","ref":"hashtree.html#destroy/1","title":"hashtree.destroy/1","type":"function"},{"doc":"","ref":"hashtree.html#flush_buffer/1","title":"hashtree.flush_buffer/1","type":"function"},{"doc":"","ref":"hashtree.html#get_bucket/3","title":"hashtree.get_bucket/3","type":"function"},{"doc":"","ref":"hashtree.html#insert/3","title":"hashtree.insert/3","type":"function"},{"doc":"","ref":"hashtree.html#insert/4","title":"hashtree.insert/4","type":"function"},{"doc":"","ref":"hashtree.html#key_hashes/2","title":"hashtree.key_hashes/2","type":"function"},{"doc":"","ref":"hashtree.html#levels/1","title":"hashtree.levels/1","type":"function"},{"doc":"","ref":"hashtree.html#mem_levels/1","title":"hashtree.mem_levels/1","type":"function"},{"doc":"","ref":"hashtree.html#new/0","title":"hashtree.new/0","type":"function"},{"doc":"","ref":"hashtree.html#new/2","title":"hashtree.new/2","type":"function"},{"doc":"","ref":"hashtree.html#new/3","title":"hashtree.new/3","type":"function"},{"doc":"","ref":"hashtree.html#read_meta/2","title":"hashtree.read_meta/2","type":"function"},{"doc":"","ref":"hashtree.html#rehash_tree/1","title":"hashtree.rehash_tree/1","type":"function"},{"doc":"","ref":"hashtree.html#segments/1","title":"hashtree.segments/1","type":"function"},{"doc":"","ref":"hashtree.html#top_hash/1","title":"hashtree.top_hash/1","type":"function"},{"doc":"","ref":"hashtree.html#update_perform/1","title":"hashtree.update_perform/1","type":"function"},{"doc":"","ref":"hashtree.html#update_snapshot/1","title":"hashtree.update_snapshot/1","type":"function"},{"doc":"","ref":"hashtree.html#update_tree/1","title":"hashtree.update_tree/1","type":"function"},{"doc":"","ref":"hashtree.html#width/1","title":"hashtree.width/1","type":"function"},{"doc":"","ref":"hashtree.html#write_meta/3","title":"hashtree.write_meta/3","type":"function"},{"doc":"","ref":"hashtree.html#t:acc_fun/1","title":"hashtree.acc_fun/1","type":"type"},{"doc":"","ref":"hashtree.html#t:bucket_bin/0","title":"hashtree.bucket_bin/0","type":"type"},{"doc":"","ref":"hashtree.html#t:hashtree/0","title":"hashtree.hashtree/0","type":"opaque"},{"doc":"","ref":"hashtree.html#t:index/0","title":"hashtree.index/0","type":"type"},{"doc":"","ref":"hashtree.html#t:keydiff/0","title":"hashtree.keydiff/0","type":"type"},{"doc":"","ref":"hashtree.html#t:meta_bin/0","title":"hashtree.meta_bin/0","type":"type"},{"doc":"","ref":"hashtree.html#t:orddict/0","title":"hashtree.orddict/0","type":"type"},{"doc":"","ref":"hashtree.html#t:proplist/0","title":"hashtree.proplist/0","type":"type"},{"doc":"","ref":"hashtree.html#t:remote_fun/0","title":"hashtree.remote_fun/0","type":"type"},{"doc":"","ref":"hashtree.html#t:segment_bin/0","title":"hashtree.segment_bin/0","type":"type"},{"doc":"","ref":"hashtree.html#t:select_fun/1","title":"hashtree.select_fun/1","type":"type"},{"doc":"","ref":"hashtree.html#t:tree_id_bin/0","title":"hashtree.tree_id_bin/0","type":"type"},{"doc":"This module implements a specialized hash tree that is used primarily by cluster metadata's anti-entropy exchanges and by metadata clients for determining when groups of metadata keys have changed locally. The tree can be used, generally, for determining the differences in groups of keys, or to find missing groups, between two stores. Each node of the tree is itself a hash tree, specifically a hashtree . The tree has a fixed height but each node has a variable amount of children. The height of the tree directly corresponds to the number of prefixes supported by the tree. A list of prefixes, or a &quot;prefix list&quot;, represent a group of keys. Each unique prefix list is a node in the tree. The leaves store hashes for the individual keys in the segments of the node's hashtree . The buckets of the leaves' hashtree provide an efficient way of determining when keys in the segments differ between two trees. The tails of the prefix list are used to roll up groups into parent groups. For example, the prefixes [a, b] , [a, c] , [d, e] will be rolled up into parent groups a , containing c and b , and d , containing only 'e'. The parent group's node has children corresponding to each child group. The top-hashes of the child nodes are stored in the parent nodes' segments. The parent nodes' buckets are used as an efficient method for determining when child groups differ between two trees. The root node corresponds to the empty list and it acts like any other node, storing hashes for the first level of child groups. The top hash of the root node is the top hash of the tree. The tree in the example above might store something like: node parent top-hash segments --------------------------------------------------- root none 1 [{a, 2}, {d, 3}] [a] root 2 [{b, 4}, {c, 5}] [d] root 3 [{e, 6}] [a,b] [a] 4 [{k1, 0}, {k2, 6}, ...] [a,c] [a] 5 [{k1, 1}, {k2, 4}, ...] [d,e] [d] 6 [{k1, 2}, {k2, 3}, ...] When a key is inserted into the tree it is inserted into the leaf corresponding to the given prefix list. The leaf and its parents are not updated at this time. Instead the leaf is added to a dirty set. The nodes are later updated in bulk. Updating the hashtree is a two step process. First, a snapshot of the tree must be obtained. This prevents new writes from affecting the update. Snapshotting the tree will snapshot each dirty leaf. Since writes to nodes other than leaves only occur during updates no snapshot is taken for them. Second, the tree is updated using the snapshot. The update is performed by updating the hashtree nodes at each level starting with the leaves. The top hash of each node in a level is inserted into its parent node after being updated. The list of dirty parents is then updated, moving up the tree. Once the root is reached and has been updated the process is complete. This process is designed to minimize the traversal of the tree and ensure that each node is only updated once. The typical use for updating a tree is to compare it with another recently updated tree. Comparison is done with the compare/4 function. Compare provides a sort of fold over the differences of the tree allowing for callers to determine what to do with those differences. In addition, the caller can accumulate a value, such as the difference list or stats about differencces. The tree implemented in this module assumes that it will be managed by a single process and that all calls will be made to it synchronously, with a couple exceptions: 1. Updating a tree with a snapshot can be done in another process. The snapshot must be taken by the owning process, synchronously. 2. Comparing two trees may be done by a seperate process. Compares should should use a snapshot and only be performed after an update. The nodes in this tree are backend by LevelDB, however, this is most likely temporary and Cluster Metadata's use of the tree is ephemeral. Trees are only meant to live for the lifetime of a running node and are rebuilt on start. To ensure the tree is fresh each time, when nodes are created the backing LevelDB store is opened, closed, and then re-opened to ensure any lingering files are removed. Additionally, the nodes themselves (references to hashtree , are stored in ets .","ref":"hashtree_tree.html","title":"hashtree_tree","type":"module"},{"doc":"Compare a local and remote tree. RemoteFun is used to access the buckets and segments of nodes in the remote tree. HandlerFun will be called for each difference found in the tree. A difference is either a missing local or remote prefix, or a list of key differences, which themselves signify different or missing keys. HandlerAcc is passed to the first call of HandlerFun and each subsequent call is passed the value returned by the previous call. The return value of this function is the return value from the last call to HandlerFun .","ref":"hashtree_tree.html#compare/4","title":"hashtree_tree.compare/4","type":"function"},{"doc":"","ref":"hashtree_tree.html#delete/3","title":"hashtree_tree.delete/3","type":"function"},{"doc":"Destroys the tree cleaning up any used resources. This deletes the LevelDB files for the nodes.","ref":"hashtree_tree.html#destroy/1","title":"hashtree_tree.destroy/1","type":"function"},{"doc":"Returns the hashtree buckets for a given node in the tree. This is used primarily for accessing buckets of a remote tree during compare.","ref":"hashtree_tree.html#get_bucket/4","title":"hashtree_tree.get_bucket/4","type":"function"},{"doc":"an alias for insert(Prefixes, Key, Hash, [], Tree)","ref":"hashtree_tree.html#insert/4","title":"hashtree_tree.insert/4","type":"function"},{"doc":"Insert a hash into the tree. The length of Prefixes must correspond to the height of the tree -- the value used for num_levels when creating the tree. The hash is inserted into a leaf of the tree and that leaf is marked as dirty. The tree is not updated at this time. Future operations on the tree should used the tree returend by this fucntion. Insert takes the following options: * if_missing - if true then the hash is only inserted into the tree if the key is not already present. This is useful for ensuring writes concurrent with building the tree take precedence over older values. false is the default value.","ref":"hashtree_tree.html#insert/5","title":"hashtree_tree.insert/5","type":"function"},{"doc":"Returns the hashtree segment hashes for a given node in the tree. This is used primarily for accessing key hashes of a remote tree during compare.","ref":"hashtree_tree.html#key_hashes/3","title":"hashtree_tree.key_hashes/3","type":"function"},{"doc":"Compare two local trees. This function is primarily for local debugging and testing.","ref":"hashtree_tree.html#local_compare/2","title":"hashtree_tree.local_compare/2","type":"function"},{"doc":"Creates a new hashtree. Takes the following options: * num_levels - the height of the tree excluding leaves. corresponds to the length of the prefix list passed to insert/5 . * data_dir - the directory where the LevelDB instances for the nodes will be stored.","ref":"hashtree_tree.html#new/2","title":"hashtree_tree.new/2","type":"function"},{"doc":"Returns the top-hash of the node corresponding to the given prefix list. The length of the prefix list can be less than or equal to the height of the tree. If the tree has not been updated or if the prefix list is not found or invalid, then undefined is returned. Otherwise the hash value from the most recent update is returned.","ref":"hashtree_tree.html#prefix_hash/2","title":"hashtree_tree.prefix_hash/2","type":"function"},{"doc":"Returns the top-hash of the tree. This is the top-hash of the root node.","ref":"hashtree_tree.html#top_hash/1","title":"hashtree_tree.top_hash/1","type":"function"},{"doc":"Update the tree with a snapshot obtained by update_snapshot/1 . This function may be called by a process other than the one managing the tree.","ref":"hashtree_tree.html#update_perform/1","title":"hashtree_tree.update_perform/1","type":"function"},{"doc":"Snapshot the tree for updating. The return tree should be updated using update_perform/1 and to perform future operations on the tree","ref":"hashtree_tree.html#update_snapshot/1","title":"hashtree_tree.update_snapshot/1","type":"function"},{"doc":"","ref":"hashtree_tree.html#t:diff/0","title":"hashtree_tree.diff/0","type":"type"},{"doc":"","ref":"hashtree_tree.html#t:handler_fun/1","title":"hashtree_tree.handler_fun/1","type":"type"},{"doc":"","ref":"hashtree_tree.html#t:insert_opt/0","title":"hashtree_tree.insert_opt/0","type":"type"},{"doc":"","ref":"hashtree_tree.html#t:insert_opt_if_missing/0","title":"hashtree_tree.insert_opt_if_missing/0","type":"type"},{"doc":"","ref":"hashtree_tree.html#t:insert_opts/0","title":"hashtree_tree.insert_opts/0","type":"type"},{"doc":"","ref":"hashtree_tree.html#t:key_diffs/0","title":"hashtree_tree.key_diffs/0","type":"type"},{"doc":"","ref":"hashtree_tree.html#t:new_opt/0","title":"hashtree_tree.new_opt/0","type":"type"},{"doc":"","ref":"hashtree_tree.html#t:new_opt_data_dir/0","title":"hashtree_tree.new_opt_data_dir/0","type":"type"},{"doc":"","ref":"hashtree_tree.html#t:new_opt_num_levels/0","title":"hashtree_tree.new_opt_num_levels/0","type":"type"},{"doc":"","ref":"hashtree_tree.html#t:new_opts/0","title":"hashtree_tree.new_opts/0","type":"type"},{"doc":"","ref":"hashtree_tree.html#t:prefix/0","title":"hashtree_tree.prefix/0","type":"type"},{"doc":"","ref":"hashtree_tree.html#t:prefix_diff/0","title":"hashtree_tree.prefix_diff/0","type":"type"},{"doc":"","ref":"hashtree_tree.html#t:prefixes/0","title":"hashtree_tree.prefixes/0","type":"type"},{"doc":"","ref":"hashtree_tree.html#t:remote_fun/0","title":"hashtree_tree.remote_fun/0","type":"type"},{"doc":"","ref":"hashtree_tree.html#t:tree/0","title":"hashtree_tree.tree/0","type":"opaque"},{"doc":"","ref":"hashtree_tree.html#t:tree_node/0","title":"hashtree_tree.tree_node/0","type":"opaque"},{"doc":"","ref":"plum_db.html","title":"plum_db","type":"module"},{"doc":"Deconstructs a broadcast that is sent using broadcast/2 as the handling module returning the message id and payload. &gt; This function is part of the implementation of the partisan_plumtree_broadcast_handler behaviour. &gt; You should never call it directly.","ref":"plum_db.html#broadcast_data/1","title":"plum_db.broadcast_data/1","type":"function"},{"doc":"Same as delete(FullPrefix, Key, [])","ref":"plum_db.html#delete/2","title":"plum_db.delete/2","type":"function"},{"doc":"Logically deletes the value associated with the given prefix and key locally and then triggers a broradcast to notify other nodes in the cluster. Currently there are no delete options. NOTE: currently deletion is logical and no GC is performed.","ref":"plum_db.html#delete/3","title":"plum_db.delete/3","type":"function"},{"doc":"Same as delete(FullPrefix, Key, [])","ref":"plum_db.html#erase/2","title":"plum_db.erase/2","type":"function"},{"doc":"Logically erases the value associated with the given prefix and key locally and then triggers a broradcast to notify other nodes in the cluster. Currently there are no erase options. NOTE: currently deletion is logical and no GC is performed.","ref":"plum_db.html#erase/3","title":"plum_db.erase/3","type":"function"},{"doc":"Triggers an asynchronous exchange. Calls exchange/2 with an empty map as the second argument. &gt; The exchange is only triggered if the application option aae_enabled is set to true .","ref":"plum_db.html#exchange/1","title":"plum_db.exchange/1","type":"function"},{"doc":"Triggers an asynchronous exchange. The exchange is performed asynchronously by spawning a supervised process. Read the plum_db_exchanges_sup documentation. Opts is a map accepting the following options: * timeout (milliseconds) –– timeout for the AAE exchange to conclude. &gt; The exchange is only triggered if the application option aae_enabled is set to true .","ref":"plum_db.html#exchange/2","title":"plum_db.exchange/2","type":"function"},{"doc":"Same as fold(Fun, Acc0, FullPrefix, []).","ref":"plum_db.html#fold/3","title":"plum_db.fold/3","type":"function"},{"doc":"Fold over all keys and values stored under a given prefix/subprefix. Available options are the same as those provided to iterator/2. To return early, throw {break, Result} in your fold function.","ref":"plum_db.html#fold/4","title":"plum_db.fold/4","type":"function"},{"doc":"Same as fold_elements(Fun, Acc0, FullPrefix, []).","ref":"plum_db.html#fold_elements/3","title":"plum_db.fold_elements/3","type":"function"},{"doc":"Fold over all elements stored under a given prefix/subprefix. Available options are the same as those provided to iterator/2. To return early, throw {break, Result} in your fold function.","ref":"plum_db.html#fold_elements/4","title":"plum_db.fold_elements/4","type":"function"},{"doc":"Same as fold(Fun, Acc0, FullPrefix, []).","ref":"plum_db.html#foreach/2","title":"plum_db.foreach/2","type":"function"},{"doc":"Fold over all keys and values stored under a given prefix/subprefix. Available options are the same as those provided to iterator/2. To return early, throw {break, Result} in your fold function.","ref":"plum_db.html#foreach/3","title":"plum_db.foreach/3","type":"function"},{"doc":"Same as get(FullPrefix, Key, [])","ref":"plum_db.html#get/2","title":"plum_db.get/2","type":"function"},{"doc":"Retrieves the local value stored at the given fullprefix FullPrefix and key Key using options Opts . Returns the stored value if found. If no value is found and Opts contains a value for the key default , this value is returned. Otherwise returns the atom undefined . Opts is a property list that can take the following options: * default – value to return if no value is found, Defaults to undefined . * resolver – The atom lww or a plum_db_resolver() that resolves conflicts if they are encountered. Defaults to lww (last-write-wins). * allow_put – whether or not to write and broadcast a resolved value. Defaults to true . Example: Simple get &gt; plum_db : get ( { foo , a } , x ) . undefined . &gt; plum_db : get ( { foo , a } , x , [ { default , 1 } ] ) . 1 . &gt; plum_db : put ( { foo , a } , x , 100 ) . ok &gt; plum_db : get ( { foo , a } , x ) . 100 . Example: Resolving with a custom function Fun = fun ( A , B ) when A &gt; B -&gt; A ; _ -&gt; B end , &gt; plum_db : get ( { foo , a } , x , [ { resolver , Fun } ] ) . &gt; NOTE: an update will be broadcasted if conflicts are resolved and allow_put is true . However, any further conflicts generated by concurrent writes during resolution are not resolved.","ref":"plum_db.html#get/3","title":"plum_db.get/3","type":"function"},{"doc":"Returns a Dotted Version Vector Set or undefined. When reading the value for a subsequent call to put/3 the context can be obtained using plum_db_object:context/1. Values can obtained w/ plum_db_object:values/1.","ref":"plum_db.html#get_object/1","title":"plum_db.get_object/1","type":"function"},{"doc":"Returns a Dotted Version Vector Set or undefined. When reading the value for a subsequent call to put/3 the context can be obtained using plum_db_object:context/1. Values can obtained w/ plum_db_object:values/1.","ref":"plum_db.html#get_object/2","title":"plum_db.get_object/2","type":"function"},{"doc":"Same as get/1 but reads the value from Node This is function is used by plum_db_exchange_statem.","ref":"plum_db.html#get_object/3","title":"plum_db.get_object/3","type":"function"},{"doc":"Returns the object associated with the given prefixed key Pkey and context Context (message id) if the currently stored version has an equal context. Otherwise returns the atom stale . Because it assumes that a grafted context can only be causally older than the local view, a stale response means there is another message that subsumes the grafted one. &gt; This function is part of the implementation of the partisan_plumtree_broadcast_handler behaviour. &gt; You should never call it directly.","ref":"plum_db.html#graft/1","title":"plum_db.graft/1","type":"function"},{"doc":"Returns true if an identifier is a valid partition.","ref":"plum_db.html#is_partition/1","title":"plum_db.is_partition/1","type":"function"},{"doc":"Returns false if the update (or a causally newer update) has already been received (stored locally). &gt; This function is part of the implementation of the partisan_plumtree_broadcast_handler behaviour. &gt; You should never call it directly.","ref":"plum_db.html#is_stale/1","title":"plum_db.is_stale/1","type":"function"},{"doc":"Advances the iterator by one key, full-prefix or sub-prefix","ref":"plum_db.html#iterate/1","title":"plum_db.iterate/1","type":"function"},{"doc":"Returns a full-prefix iterator: an iterator for all full-prefixes that have keys stored under them. When done with the iterator, iterator_close/1 must be called. This iterator works across all existing store partitions, treating the set of partitions as a single logical database. As a result, ordering is partial per partition and not global across them. Same as calling iterator({undefined, undefined}) .","ref":"plum_db.html#iterator/0","title":"plum_db.iterator/0","type":"function"},{"doc":"Same as calling iterator(FullPrefix, []) .","ref":"plum_db.html#iterator/1","title":"plum_db.iterator/1","type":"function"},{"doc":"Return an iterator pointing to the first key stored under a prefix This function can take the following options: * resolver : either the atom lww or a function that resolves conflicts if they are encounted (see get/3 for more details). Conflict resolution is performed when values are retrieved (see iterator_key_value/1 and iterator_key_values/1). If no resolver is provided no resolution is performed. The default is to not provide a resolver. * allow_put : whether or not to write and broadcast a resolved value. defaults to true . * default : Used when the value an iterator points to is a tombstone. default is either an arity-1 function or a value. If a function, the key the iterator points to is passed as the argument and the result is returned in place of the tombstone. If default is a value, the value is returned in place of the tombstone. This applies when using functions such as iterator_key_values/1 and iterator_key_values/1. * first - the key this iterator should start at, equivalent to calling iterator_move/2 passing the key as the second argument. * match : If match is undefined then all keys will may be visted by the iterator, match can be: * an erlang term - which will be matched exactly against a key * '_' - the wildcard term which matches anything * an erlang tuple containing terms and '_' - if tuples are used as keys this can be used to iterate over some subset of keys * partitions : The list of partitions this iterator should cover. If undefined it will cover all partitions ( pdb:partitions/0 ) * keys_only : wether to iterate only on keys (default: false)","ref":"plum_db.html#iterator/2","title":"plum_db.iterator/2","type":"function"},{"doc":"Closes the iterator. This function must be called on all open iterators","ref":"plum_db.html#iterator_close/1","title":"plum_db.iterator_close/1","type":"function"},{"doc":"Returns the value returned when an iterator points to a tombstone. If the default used when creating the given iterator is a function it will be applied to the current key the iterator points at. If no default was provided the tombstone value was returned. This function should only be called after checking iterator_done/1.","ref":"plum_db.html#iterator_default/1","title":"plum_db.iterator_default/1","type":"function"},{"doc":"Returns true if there is nothing more to iterate over","ref":"plum_db.html#iterator_done/1","title":"plum_db.iterator_done/1","type":"function"},{"doc":"","ref":"plum_db.html#iterator_element/1","title":"plum_db.iterator_element/1","type":"function"},{"doc":"Return the key pointed at by the iterator. Before calling this function, check the iterator is not complete w/ iterator_done/1. No conflict resolution will be performed as a result of calling this function.","ref":"plum_db.html#iterator_key/1","title":"plum_db.iterator_key/1","type":"function"},{"doc":"Returns a single value pointed at by the iterator. If there are conflicts and a resolver was specified in the options when creating this iterator, they will be resolved. Otherwise, and error is returned. If conflicts are resolved, the resolved value is written locally and a broadcast is performed to update other nodes in the cluster if allow_put is true (the default value). If allow_put is false , values are resolved but not written or broadcast. NOTE: if resolution may be performed this function must be called at most once before calling iterate/1 on the iterator (at which point the function can be called once more).","ref":"plum_db.html#iterator_key_value/1","title":"plum_db.iterator_key_value/1","type":"function"},{"doc":"Return the key and all sibling values pointed at by the iterator. Before calling this function, check the iterator is not complete w/ iterator_done/1. If a resolver was passed to iterator/0 when creating the given iterator, siblings will be resolved using the given function or last-write-wins (if lww is passed as the resolver). If no resolver was used then no conflict resolution will take place. If conflicts are resolved, the resolved value is written to local store and a broadcast is submitted to update other nodes in the cluster if allow_put is true . If allow_put is false the values are resolved but are not written or broadcast. A single value is returned as the second element of the tuple in the case values are resolved. If no resolution takes place then a list of values will be returned as the second element (even if there is only a single sibling). NOTE: if resolution may be performed this function must be called at most once before calling iterate/1 on the iterator (at which point the function can be called once more).","ref":"plum_db.html#iterator_key_values/1","title":"plum_db.iterator_key_values/1","type":"function"},{"doc":"Returns the full-prefix being iterated by this iterator.","ref":"plum_db.html#iterator_prefix/1","title":"plum_db.iterator_prefix/1","type":"function"},{"doc":"","ref":"plum_db.html#match/1","title":"plum_db.match/1","type":"function"},{"doc":"","ref":"plum_db.html#match/2","title":"plum_db.match/2","type":"function"},{"doc":"","ref":"plum_db.html#match/3","title":"plum_db.match/3","type":"function"},{"doc":"Merges a remote copy of an object record sent via broadcast w/ the local view for the key contained in the message id. If the remote copy is causally older than the current data stored then false is returned and no updates are merged. Otherwise, the remote copy is merged (possibly generating siblings) and true is returned. &gt; This function is part of the implementation of the partisan_plumtree_broadcast_handler behaviour. &gt; You should never call it directly.","ref":"plum_db.html#merge/2","title":"plum_db.merge/2","type":"function"},{"doc":"Same as merge/2 but merges the object on Node &gt; This function is part of the implementation of the partisan_plumtree_broadcast_handler behaviour. &gt; You should never call it directly.","ref":"plum_db.html#merge/3","title":"plum_db.merge/3","type":"function"},{"doc":"Returns the number of partitions.","ref":"plum_db.html#partition_count/0","title":"plum_db.partition_count/0","type":"function"},{"doc":"Returns the list of the partition identifiers starting at 0.","ref":"plum_db.html#partitions/0","title":"plum_db.partitions/0","type":"function"},{"doc":"Return the local hash associated with a full-prefix or prefix. The hash value is updated periodically and does not always reflect the most recent value. This function can be used to determine when keys stored under a full-prefix or prefix have changed. If the tree has not yet been updated or there are no keys stored the given (full-)prefix. undefined is returned.","ref":"plum_db.html#prefix_hash/2","title":"plum_db.prefix_hash/2","type":"function"},{"doc":"","ref":"plum_db.html#prefix_type/1","title":"plum_db.prefix_type/1","type":"function"},{"doc":"Returns a mapping of prefixes (the first element of a plum_db_prefix() tuple) to prefix_type() only for those prefixes for which a type was declared using the application optiont prefixes .","ref":"plum_db.html#prefixes/0","title":"plum_db.prefixes/0","type":"function"},{"doc":"Same as put(FullPrefix, Key, Value, [])","ref":"plum_db.html#put/3","title":"plum_db.put/3","type":"function"},{"doc":"Stores or updates the value at the given prefix and key locally and then triggers a broadcast to notify other nodes in the cluster. Currently, there are no put options. NOTE: because the third argument to this function can be a plum_db_modifier(), used to resolve conflicts on write, values cannot be functions. To store functions wrap them in another type like a tuple.","ref":"plum_db.html#put/4","title":"plum_db.put/4","type":"function"},{"doc":"Create an iterator on Node . This allows for remote iteration by having the worker keep track of the actual iterator (since ets continuations cannot cross node boundaries). The iterator created iterates all full-prefixes. Once created the rest of the iterator API may be used as usual. When done with the iterator, iterator_close/1 must be called","ref":"plum_db.html#remote_iterator/1","title":"plum_db.remote_iterator/1","type":"function"},{"doc":"","ref":"plum_db.html#remote_iterator/2","title":"plum_db.remote_iterator/2","type":"function"},{"doc":"Start the plum_db server and link to calling process. The plum_db server is responsible for managing local and remote iterators. No API function uses the server itself.","ref":"plum_db.html#start_link/0","title":"plum_db.start_link/0","type":"function"},{"doc":"Triggers a synchronous exchange. Calls sync_exchange/2 with an empty map as the second argument. &gt; The exchange is only triggered if the application option aae_enabled is set to true .","ref":"plum_db.html#sync_exchange/1","title":"plum_db.sync_exchange/1","type":"function"},{"doc":"Triggers a synchronous exchange. The exchange is performed synchronously by spawning a supervised process and waiting (blocking) till it finishes. Read the plum_db_exchanges_sup documentation. Opts is a map accepting the following options: * timeout (milliseconds) –– timeout for the AAE exchange to conclude. &gt; The exchange is only triggered if the application option aae_enabled is set to true .","ref":"plum_db.html#sync_exchange/2","title":"plum_db.sync_exchange/2","type":"function"},{"doc":"","ref":"plum_db.html#take/2","title":"plum_db.take/2","type":"function"},{"doc":"","ref":"plum_db.html#take/3","title":"plum_db.take/3","type":"function"},{"doc":"Same as to_list(FullPrefix, [])","ref":"plum_db.html#to_list/1","title":"plum_db.to_list/1","type":"function"},{"doc":"Return a list of all keys and values stored under a given prefix/subprefix. Available options are the same as those provided to iterator/2.","ref":"plum_db.html#to_list/2","title":"plum_db.to_list/2","type":"function"},{"doc":"","ref":"plum_db.html#t:continuation/0","title":"plum_db.continuation/0","type":"opaque"},{"doc":"","ref":"plum_db.html#t:continuation_or_eot/0","title":"plum_db.continuation_or_eot/0","type":"type"},{"doc":"","ref":"plum_db.html#t:delete_opts/0","title":"plum_db.delete_opts/0","type":"type"},{"doc":"","ref":"plum_db.html#t:eot/0","title":"plum_db.eot/0","type":"type"},{"doc":"","ref":"plum_db.html#t:erase_opts/0","title":"plum_db.erase_opts/0","type":"type"},{"doc":"","ref":"plum_db.html#t:fold_elements_fun/0","title":"plum_db.fold_elements_fun/0","type":"type"},{"doc":"","ref":"plum_db.html#t:fold_fun/0","title":"plum_db.fold_fun/0","type":"type"},{"doc":"","ref":"plum_db.html#t:fold_opts/0","title":"plum_db.fold_opts/0","type":"type"},{"doc":"","ref":"plum_db.html#t:foreach_fun/0","title":"plum_db.foreach_fun/0","type":"type"},{"doc":"","ref":"plum_db.html#t:get_opt/0","title":"plum_db.get_opt/0","type":"type"},{"doc":"","ref":"plum_db.html#t:get_opt_allow_put/0","title":"plum_db.get_opt_allow_put/0","type":"type"},{"doc":"","ref":"plum_db.html#t:get_opt_default_val/0","title":"plum_db.get_opt_default_val/0","type":"type"},{"doc":"","ref":"plum_db.html#t:get_opt_resolver/0","title":"plum_db.get_opt_resolver/0","type":"type"},{"doc":"","ref":"plum_db.html#t:get_opts/0","title":"plum_db.get_opts/0","type":"type"},{"doc":"","ref":"plum_db.html#t:it_opt/0","title":"plum_db.it_opt/0","type":"type"},{"doc":"","ref":"plum_db.html#t:it_opt_default/0","title":"plum_db.it_opt_default/0","type":"type"},{"doc":"","ref":"plum_db.html#t:it_opt_default_fun/0","title":"plum_db.it_opt_default_fun/0","type":"type"},{"doc":"","ref":"plum_db.html#t:it_opt_first/0","title":"plum_db.it_opt_first/0","type":"type"},{"doc":"","ref":"plum_db.html#t:it_opt_keymatch/0","title":"plum_db.it_opt_keymatch/0","type":"type"},{"doc":"","ref":"plum_db.html#t:it_opt_keys_only/0","title":"plum_db.it_opt_keys_only/0","type":"type"},{"doc":"","ref":"plum_db.html#t:it_opt_partitions/0","title":"plum_db.it_opt_partitions/0","type":"type"},{"doc":"","ref":"plum_db.html#t:it_opt_resolver/0","title":"plum_db.it_opt_resolver/0","type":"type"},{"doc":"","ref":"plum_db.html#t:it_opts/0","title":"plum_db.it_opts/0","type":"type"},{"doc":"","ref":"plum_db.html#t:iterator/0","title":"plum_db.iterator/0","type":"opaque"},{"doc":"","ref":"plum_db.html#t:iterator_element/0","title":"plum_db.iterator_element/0","type":"type"},{"doc":"","ref":"plum_db.html#t:match_opt_limit/0","title":"plum_db.match_opt_limit/0","type":"type"},{"doc":"","ref":"plum_db.html#t:match_opt_remove_tombstones/0","title":"plum_db.match_opt_remove_tombstones/0","type":"type"},{"doc":"","ref":"plum_db.html#t:match_opts/0","title":"plum_db.match_opts/0","type":"type"},{"doc":"","ref":"plum_db.html#t:partition/0","title":"plum_db.partition/0","type":"type"},{"doc":"","ref":"plum_db.html#t:plum_db_broadcast/0","title":"plum_db.plum_db_broadcast/0","type":"type"},{"doc":"","ref":"plum_db.html#t:plum_db_context/0","title":"plum_db.plum_db_context/0","type":"type"},{"doc":"","ref":"plum_db.html#t:plum_db_key/0","title":"plum_db.plum_db_key/0","type":"type"},{"doc":"","ref":"plum_db.html#t:plum_db_modifier/0","title":"plum_db.plum_db_modifier/0","type":"type"},{"doc":"","ref":"plum_db.html#t:plum_db_object/0","title":"plum_db.plum_db_object/0","type":"type"},{"doc":"","ref":"plum_db.html#t:plum_db_pkey/0","title":"plum_db.plum_db_pkey/0","type":"type"},{"doc":"","ref":"plum_db.html#t:plum_db_pkey_pattern/0","title":"plum_db.plum_db_pkey_pattern/0","type":"type"},{"doc":"","ref":"plum_db.html#t:plum_db_prefix/0","title":"plum_db.plum_db_prefix/0","type":"type"},{"doc":"","ref":"plum_db.html#t:plum_db_prefix_pattern/0","title":"plum_db.plum_db_prefix_pattern/0","type":"type"},{"doc":"","ref":"plum_db.html#t:plum_db_resolver/0","title":"plum_db.plum_db_resolver/0","type":"type"},{"doc":"","ref":"plum_db.html#t:plum_db_tombstone/0","title":"plum_db.plum_db_tombstone/0","type":"type"},{"doc":"","ref":"plum_db.html#t:plum_db_value/0","title":"plum_db.plum_db_value/0","type":"type"},{"doc":"","ref":"plum_db.html#t:plum_db_wildcard/0","title":"plum_db.plum_db_wildcard/0","type":"type"},{"doc":"","ref":"plum_db.html#t:prefix_type/0","title":"plum_db.prefix_type/0","type":"type"},{"doc":"","ref":"plum_db.html#t:prefixes/0","title":"plum_db.prefixes/0","type":"type"},{"doc":"","ref":"plum_db.html#t:put_opts/0","title":"plum_db.put_opts/0","type":"type"},{"doc":"","ref":"plum_db.html#t:remote_iterator/0","title":"plum_db.remote_iterator/0","type":"type"},{"doc":"","ref":"plum_db.html#t:state/0","title":"plum_db.state/0","type":"type"},{"doc":"","ref":"plum_db.html#t:value_or_values/0","title":"plum_db.value_or_values/0","type":"type"},{"doc":"+ -- -- -- -- -- -- -- -- -- + | | | plum_db_sup | | | + -- -- -- -- -- -- -- -- -- + | + -- -- -- -- -- -- -- -- -- -- -- - + -- -- -- -- -- -- -- -- -- -- -- - + | | | v v v + -- -- -- -- -- -- -- -- -- + + -- -- -- -- -- -- -- -- -- + + -- -- -- -- -- -- -- -- -- + | | | plum_db_ | | | | plum_db | | partitions_sup | | plum_db_events | | | | | | | + -- -- -- -- -- -- -- -- -- + + -- -- -- -- -- -- -- -- -- + + -- -- -- -- -- -- -- -- -- + | + -- -- -- -- -- - + -- -- -- -- -- - + | | v v + -- -- -- -- -- -- -- -- -- + + -- -- -- -- -- -- -- -- -- + | plum_db_partition_ | | plum_db_partition_ | | 1 _ sup | | n_sup | | | | | + -- -- -- -- -- -- -- -- -- + + -- -- -- -- -- -- -- -- -- + | + -- -- -- -- -- - + -- -- -- -- -- -- -- -- -- -- -- + | | v v + -- -- -- -- -- -- -- -- -- + + -- -- -- -- -- -- -- -- -- + | plum_db_partition_ | | plum_db_partition_ | | 1 _ server | | 1 _ hashtree | | | | | + -- -- -- -- -- -- -- -- -- + + -- -- -- -- -- -- -- -- -- + | | v v + - - - - - - - - - + - - - - - - - - - | | | eleveldb | eleveldb | | + - - - - - - - - - + - - - - - - - - -","ref":"plum_db_app.html","title":"plum_db_app","type":"module"},{"doc":"","ref":"plum_db_app.html#prep_stop/1","title":"plum_db_app.prep_stop/1","type":"function"},{"doc":"","ref":"plum_db_app.html#start/2","title":"plum_db_app.start/2","type":"function"},{"doc":"Application behaviour callback","ref":"plum_db_app.html#start_phase/3","title":"plum_db_app.start_phase/3","type":"function"},{"doc":"","ref":"plum_db_app.html#stop/1","title":"plum_db_app.stop/1","type":"function"},{"doc":"","ref":"plum_db_config.html","title":"plum_db_config","type":"module"},{"doc":"","ref":"plum_db_config.html#get/1","title":"plum_db_config.get/1","type":"function"},{"doc":"","ref":"plum_db_config.html#get/2","title":"plum_db_config.get/2","type":"function"},{"doc":"Initialises plum_db configuration","ref":"plum_db_config.html#init/0","title":"plum_db_config.init/0","type":"function"},{"doc":"","ref":"plum_db_config.html#on_set/2","title":"plum_db_config.on_set/2","type":"function"},{"doc":"","ref":"plum_db_config.html#set/2","title":"plum_db_config.set/2","type":"function"},{"doc":"","ref":"plum_db_config.html#will_set/2","title":"plum_db_config.will_set/2","type":"function"},{"doc":"","ref":"plum_db_console.html","title":"plum_db_console","type":"module"},{"doc":"","ref":"plum_db_console.html#members/1","title":"plum_db_console.members/1","type":"function"},{"doc":"An Erlang implementation of *compact* Dotted Version Vectors, which provides a container for a set of concurrent values (siblings) with causal order information. For further reading, visit the github page .","ref":"plum_db_dvvset.html","title":"plum_db_dvvset","type":"module"},{"doc":"Compares the equality of both clocks, regarding only the causal histories, thus ignoring the values.","ref":"plum_db_dvvset.html#equal/2","title":"plum_db_dvvset.equal/2","type":"function"},{"doc":"Returns all the ids used in this clock set.","ref":"plum_db_dvvset.html#ids/1","title":"plum_db_dvvset.ids/1","type":"function"},{"doc":"Return a version vector that represents the causal history.","ref":"plum_db_dvvset.html#join/1","title":"plum_db_dvvset.join/1","type":"function"},{"doc":"Returns the latest value in the clock set, according to function F(A,B), which returns *true* if A compares less than or equal to B, false otherwise.","ref":"plum_db_dvvset.html#last/2","title":"plum_db_dvvset.last/2","type":"function"},{"doc":"Returns True if the first clock is causally older than the second clock, thus values on the first clock are outdated. Returns False otherwise.","ref":"plum_db_dvvset.html#less/2","title":"plum_db_dvvset.less/2","type":"function"},{"doc":"Return a clock with the same causal history, but with only one value in its original position. This value is the newest value in the given clock, according to function F(A,B), which returns *true* if A compares less than or equal to B, false otherwise.","ref":"plum_db_dvvset.html#lww/2","title":"plum_db_dvvset.lww/2","type":"function"},{"doc":"Maps (applies) a function on all values in this clock set, returning the same clock set with the updated values.","ref":"plum_db_dvvset.html#map/2","title":"plum_db_dvvset.map/2","type":"function"},{"doc":"Constructs a new clock set without causal history, and receives a list of values that gos to the anonymous list.","ref":"plum_db_dvvset.html#new/1","title":"plum_db_dvvset.new/1","type":"function"},{"doc":"Constructs a new clock set with the causal history of the given version vector / vector clock, and receives a list of values that gos to the anonymous list. The version vector SHOULD BE a direct result of join/1.","ref":"plum_db_dvvset.html#new/2","title":"plum_db_dvvset.new/2","type":"function"},{"doc":"Return a clock with the same causal history, but with only one value in the anonymous placeholder. This value is the result of the function F, which takes all values and returns a single new value.","ref":"plum_db_dvvset.html#reconcile/2","title":"plum_db_dvvset.reconcile/2","type":"function"},{"doc":"Returns the total number of values in this clock set.","ref":"plum_db_dvvset.html#size/1","title":"plum_db_dvvset.size/1","type":"function"},{"doc":"Synchronizes a list of clocks using sync/2. It discards (causally) outdated values, while merging all causal histories.","ref":"plum_db_dvvset.html#sync/1","title":"plum_db_dvvset.sync/1","type":"function"},{"doc":"Advances the causal history with the given id. The new value is the *anonymous dot* of the clock. The client clock SHOULD BE a direct result of new/2.","ref":"plum_db_dvvset.html#update/2","title":"plum_db_dvvset.update/2","type":"function"},{"doc":"Advances the causal history of the first clock with the given id, while synchronizing with the second clock, thus the new clock is causally newer than both clocks in the argument. The new value is the *anonymous dot* of the clock. The first clock SHOULD BE a direct result of new/2, which is intended to be the client clock with the new value in the *anonymous dot* while the second clock is from the local server.","ref":"plum_db_dvvset.html#update/3","title":"plum_db_dvvset.update/3","type":"function"},{"doc":"Returns all the values used in this clock set, including the anonymous values.","ref":"plum_db_dvvset.html#values/1","title":"plum_db_dvvset.values/1","type":"function"},{"doc":"","ref":"plum_db_dvvset.html#t:clock/0","title":"plum_db_dvvset.clock/0","type":"type"},{"doc":"","ref":"plum_db_dvvset.html#t:counter/0","title":"plum_db_dvvset.counter/0","type":"type"},{"doc":"","ref":"plum_db_dvvset.html#t:entries/0","title":"plum_db_dvvset.entries/0","type":"type"},{"doc":"","ref":"plum_db_dvvset.html#t:id/0","title":"plum_db_dvvset.id/0","type":"type"},{"doc":"","ref":"plum_db_dvvset.html#t:value/0","title":"plum_db_dvvset.value/0","type":"type"},{"doc":"","ref":"plum_db_dvvset.html#t:values/0","title":"plum_db_dvvset.values/0","type":"type"},{"doc":"","ref":"plum_db_dvvset.html#t:vector/0","title":"plum_db_dvvset.vector/0","type":"type"},{"doc":"","ref":"plum_db_events.html","title":"plum_db_events","type":"module"},{"doc":"Adds a callback function. The function needs to have a single argument representing the event that has been fired.","ref":"plum_db_events.html#add_callback/1","title":"plum_db_events.add_callback/1","type":"function"},{"doc":"Adds an event handler. Calls gen_event:add_handler(?MODULE, Handler, Args) .","ref":"plum_db_events.html#add_handler/2","title":"plum_db_events.add_handler/2","type":"function"},{"doc":"Adds a supervised callback function. The function needs to have a single argument representing the event that has been fired.","ref":"plum_db_events.html#add_sup_callback/1","title":"plum_db_events.add_sup_callback/1","type":"function"},{"doc":"Adds a supervised event handler. Calls gen_event:add_sup_handler(?MODULE, Handler, Args) .","ref":"plum_db_events.html#add_sup_handler/2","title":"plum_db_events.add_sup_handler/2","type":"function"},{"doc":"","ref":"plum_db_events.html#code_change/3","title":"plum_db_events.code_change/3","type":"function"},{"doc":"","ref":"plum_db_events.html#delete_handler/1","title":"plum_db_events.delete_handler/1","type":"function"},{"doc":"","ref":"plum_db_events.html#delete_handler/2","title":"plum_db_events.delete_handler/2","type":"function"},{"doc":"","ref":"plum_db_events.html#handle_call/2","title":"plum_db_events.handle_call/2","type":"function"},{"doc":"","ref":"plum_db_events.html#handle_event/2","title":"plum_db_events.handle_event/2","type":"function"},{"doc":"","ref":"plum_db_events.html#handle_info/2","title":"plum_db_events.handle_info/2","type":"function"},{"doc":"","ref":"plum_db_events.html#init/1","title":"plum_db_events.init/1","type":"function"},{"doc":"","ref":"plum_db_events.html#notify/2","title":"plum_db_events.notify/2","type":"function"},{"doc":"","ref":"plum_db_events.html#start_link/0","title":"plum_db_events.start_link/0","type":"function"},{"doc":"Subscribe to events of type Event. Any events published through update/1 will delivered to the calling process, along with all other subscribers. This function will raise an exception if you try to subscribe to the same event twice from the same process. This function uses plum_db_pubsub:subscribe/2.","ref":"plum_db_events.html#subscribe/1","title":"plum_db_events.subscribe/1","type":"function"},{"doc":"Subscribe conditionally to events of type Event. This function is similar to subscribe/2, but adds a condition in the form of an ets match specification. The condition is tested and a message is delivered only if the condition is true. Specifically, the test is: ets:match_spec_run([Msg], ets:match_spec_compile(Cond)) == [true] In other words, if the match_spec returns true for a message, that message is sent to the subscriber. For any other result from the match_spec, the message is not sent. Cond == undefined means that all messages will be delivered, which means that Cond=undefined and Cond=[{ _',[],[true]}]' are equivalent. This function will raise an exception if you try to subscribe to the same event twice from the same process. This function uses plum_db_pubsub:subscribe_cond/2 .","ref":"plum_db_events.html#subscribe/2","title":"plum_db_events.subscribe/2","type":"function"},{"doc":"","ref":"plum_db_events.html#terminate/2","title":"plum_db_events.terminate/2","type":"function"},{"doc":"Remove subscription created using subscribe/1,2","ref":"plum_db_events.html#unsubscribe/1","title":"plum_db_events.unsubscribe/1","type":"function"},{"doc":"Notify the event handlers, callback funs and subscribers of an updated object. The message delivered to each subscriber will be of the form: {plum_db_event, Event, Msg}","ref":"plum_db_events.html#update/1","title":"plum_db_events.update/1","type":"function"},{"doc":"","ref":"plum_db_exchange_statem.html","title":"plum_db_exchange_statem","type":"module"},{"doc":"","ref":"plum_db_exchange_statem.html#acquiring_locks/3","title":"plum_db_exchange_statem.acquiring_locks/3","type":"function"},{"doc":"","ref":"plum_db_exchange_statem.html#callback_mode/0","title":"plum_db_exchange_statem.callback_mode/0","type":"function"},{"doc":"","ref":"plum_db_exchange_statem.html#code_change/4","title":"plum_db_exchange_statem.code_change/4","type":"function"},{"doc":"","ref":"plum_db_exchange_statem.html#exchanging_data/3","title":"plum_db_exchange_statem.exchanging_data/3","type":"function"},{"doc":"","ref":"plum_db_exchange_statem.html#init/1","title":"plum_db_exchange_statem.init/1","type":"function"},{"doc":"Start an exchange of plum_db hashtrees between this node and Peer for a given Partition . Timeout is the number of milliseconds the process will wait to aqcuire the remote lock or to update both trees.","ref":"plum_db_exchange_statem.html#start/2","title":"plum_db_exchange_statem.start/2","type":"function"},{"doc":"","ref":"plum_db_exchange_statem.html#start_link/2","title":"plum_db_exchange_statem.start_link/2","type":"function"},{"doc":"","ref":"plum_db_exchange_statem.html#terminate/3","title":"plum_db_exchange_statem.terminate/3","type":"function"},{"doc":"","ref":"plum_db_exchange_statem.html#updating_hashtrees/3","title":"plum_db_exchange_statem.updating_hashtrees/3","type":"function"},{"doc":"","ref":"plum_db_exchanges_sup.html","title":"plum_db_exchanges_sup","type":"module"},{"doc":"","ref":"plum_db_exchanges_sup.html#init/1","title":"plum_db_exchanges_sup.init/1","type":"function"},{"doc":"Starts a new exchange provided we would not reach the limit set by the aae_concurrency config parameter. If the limit is reached returns the error tuple {error, concurrency_limit}","ref":"plum_db_exchanges_sup.html#start_exchange/2","title":"plum_db_exchanges_sup.start_exchange/2","type":"function"},{"doc":"","ref":"plum_db_exchanges_sup.html#start_link/0","title":"plum_db_exchanges_sup.start_link/0","type":"function"},{"doc":"","ref":"plum_db_exchanges_sup.html#stop_exchange/1","title":"plum_db_exchanges_sup.stop_exchange/1","type":"function"},{"doc":"","ref":"plum_db_object.html","title":"plum_db_object","type":"module"},{"doc":"returns the context (opaque causal history) for the given object","ref":"plum_db_object.html#context/1","title":"plum_db_object.context/1","type":"function"},{"doc":"returns the representation for an empty context (opaque causal history)","ref":"plum_db_object.html#empty_context/0","title":"plum_db_object.empty_context/0","type":"function"},{"doc":"Returns true if the given context and the context of the existing object are equal","ref":"plum_db_object.html#equal_context/2","title":"plum_db_object.equal_context/2","type":"function"},{"doc":"returns a hash representing the objects contents","ref":"plum_db_object.html#hash/1","title":"plum_db_object.hash/1","type":"function"},{"doc":"Determines if the given context (version vector) is causually newer than an existing object. If the object missing or if the context does not represent an anscestor of the current key, false is returned. Otherwise, when the context does represent an ancestor of the existing object or the existing object itself, true is returned","ref":"plum_db_object.html#is_stale/2","title":"plum_db_object.is_stale/2","type":"function"},{"doc":"modifies a potentially existing object, setting its value and updating the causual history. If a function is provided as the third argument then this function also is used for conflict resolution. The difference between this function and resolve/2 is that the logical clock is advanced in the case of this function. Additionally, the resolution functions are slightly different.","ref":"plum_db_object.html#modify/4","title":"plum_db_object.modify/4","type":"function"},{"doc":"Reconciles a remote object received during replication or anti-entropy with a local object. If the remote object is an anscestor of or is equal to the local one false is returned, otherwise the reconciled object is returned as the second element of the two-tuple","ref":"plum_db_object.html#reconcile/2","title":"plum_db_object.reconcile/2","type":"function"},{"doc":"Resolves siblings using either last-write-wins or the provided function and returns an object containing a single value. The causal history is not updated","ref":"plum_db_object.html#resolve/2","title":"plum_db_object.resolve/2","type":"function"},{"doc":"returns a single value. if the object holds more than one value an error is generated See also: values/2.","ref":"plum_db_object.html#value/1","title":"plum_db_object.value/1","type":"function"},{"doc":"returns the number of siblings in the given object","ref":"plum_db_object.html#value_count/1","title":"plum_db_object.value_count/1","type":"function"},{"doc":"returns a list of values held in the object","ref":"plum_db_object.html#values/1","title":"plum_db_object.values/1","type":"function"},{"doc":"","ref":"plum_db_object.html#t:plum_db_context/0","title":"plum_db_object.plum_db_context/0","type":"type"},{"doc":"","ref":"plum_db_object.html#t:plum_db_modifier/0","title":"plum_db_object.plum_db_modifier/0","type":"type"},{"doc":"","ref":"plum_db_object.html#t:plum_db_object/0","title":"plum_db_object.plum_db_object/0","type":"type"},{"doc":"","ref":"plum_db_object.html#t:plum_db_tombstone/0","title":"plum_db_object.plum_db_tombstone/0","type":"type"},{"doc":"","ref":"plum_db_object.html#t:plum_db_value/0","title":"plum_db_object.plum_db_value/0","type":"type"},{"doc":"","ref":"plum_db_partition_hashtree.html","title":"plum_db_partition_hashtree","type":"module"},{"doc":"","ref":"plum_db_partition_hashtree.html#code_change/3","title":"plum_db_partition_hashtree.code_change/3","type":"function"},{"doc":"Compare the local tree managed by this process with the remote tree also managed by a hashtree process. RemoteFun is used to access the buckets and segments of nodes in the remote tree and should usually call get_bucket/5 and key_hashes/4 . HandlerFun is used to process the differences found between the two trees. HandlerAcc is passed to the first invocation of HandlerFun . Subsequent calls are passed the return value from the previous call. This function returns the return value from the last call to HandlerFun . hashtree_tree for more details on RemoteFun , HandlerFun and HandlerAcc .","ref":"plum_db_partition_hashtree.html#compare/4","title":"plum_db_partition_hashtree.compare/4","type":"function"},{"doc":"Same as insert(PKey, Hash, false).","ref":"plum_db_partition_hashtree.html#delete/1","title":"plum_db_partition_hashtree.delete/1","type":"function"},{"doc":"Same as insert(PKey, Hash, false).","ref":"plum_db_partition_hashtree.html#delete/2","title":"plum_db_partition_hashtree.delete/2","type":"function"},{"doc":"Return the bucket for a node in the tree managed by this process running on Node .","ref":"plum_db_partition_hashtree.html#get_bucket/5","title":"plum_db_partition_hashtree.get_bucket/5","type":"function"},{"doc":"","ref":"plum_db_partition_hashtree.html#handle_call/3","title":"plum_db_partition_hashtree.handle_call/3","type":"function"},{"doc":"","ref":"plum_db_partition_hashtree.html#handle_cast/2","title":"plum_db_partition_hashtree.handle_cast/2","type":"function"},{"doc":"","ref":"plum_db_partition_hashtree.html#handle_info/2","title":"plum_db_partition_hashtree.handle_info/2","type":"function"},{"doc":"","ref":"plum_db_partition_hashtree.html#init/1","title":"plum_db_partition_hashtree.init/1","type":"function"},{"doc":"Same as insert(PKey, Hash, false).","ref":"plum_db_partition_hashtree.html#insert/2","title":"plum_db_partition_hashtree.insert/2","type":"function"},{"doc":"Insert a hash for a full-prefix and key into the tree managed by the process. If IfMissing is true the hash is only inserted into the tree if the key is not already present.","ref":"plum_db_partition_hashtree.html#insert/3","title":"plum_db_partition_hashtree.insert/3","type":"function"},{"doc":"","ref":"plum_db_partition_hashtree.html#insert/4","title":"plum_db_partition_hashtree.insert/4","type":"function"},{"doc":"Return the key hashes for a node in the tree managed by this process running on Node .","ref":"plum_db_partition_hashtree.html#key_hashes/4","title":"plum_db_partition_hashtree.key_hashes/4","type":"function"},{"doc":"Locks the tree on this node for updating on behalf of the calling process. See also: lock/3 .","ref":"plum_db_partition_hashtree.html#lock/1","title":"plum_db_partition_hashtree.lock/1","type":"function"},{"doc":"Locks the tree on Node for updating on behalf of the calling process. See also: lock/3 .","ref":"plum_db_partition_hashtree.html#lock/2","title":"plum_db_partition_hashtree.lock/2","type":"function"},{"doc":"Lock the tree for updating. This function must be called before updating the tree with update/1 or update/2 . If the tree is not built or already locked then the call will fail and the appropriate atom is returned. Otherwise, aqcuiring the lock succeeds and ok is returned.","ref":"plum_db_partition_hashtree.html#lock/3","title":"plum_db_partition_hashtree.lock/3","type":"function"},{"doc":"","ref":"plum_db_partition_hashtree.html#name/1","title":"plum_db_partition_hashtree.name/1","type":"function"},{"doc":"Return the hash for the given prefix or full-prefix","ref":"plum_db_partition_hashtree.html#prefix_hash/2","title":"plum_db_partition_hashtree.prefix_hash/2","type":"function"},{"doc":"Locks the tree on this node for updating on behalf of the calling process. See also: lock/3 .","ref":"plum_db_partition_hashtree.html#release_lock/1","title":"plum_db_partition_hashtree.release_lock/1","type":"function"},{"doc":"Locks the tree on Node for updating on behalf of the calling process. See also: lock/3 .","ref":"plum_db_partition_hashtree.html#release_lock/2","title":"plum_db_partition_hashtree.release_lock/2","type":"function"},{"doc":"Lock the tree for updating. This function must be called before updating the tree with update/1 or update/2 . If the tree is not built or already locked then the call will fail and the appropriate atom is returned. Otherwise, aqcuiring the lock succeeds and ok is returned.","ref":"plum_db_partition_hashtree.html#release_lock/3","title":"plum_db_partition_hashtree.release_lock/3","type":"function"},{"doc":"Destroys the partition's hashtrees on the local node and rebuilds it.","ref":"plum_db_partition_hashtree.html#reset/1","title":"plum_db_partition_hashtree.reset/1","type":"function"},{"doc":"Destroys the partition's hashtrees on node Node and rebuilds it.","ref":"plum_db_partition_hashtree.html#reset/2","title":"plum_db_partition_hashtree.reset/2","type":"function"},{"doc":"Starts the process using start_link/1 , passing in the directory where other cluster data is stored in hashtrees_dir as the data root.","ref":"plum_db_partition_hashtree.html#start_link/1","title":"plum_db_partition_hashtree.start_link/1","type":"function"},{"doc":"","ref":"plum_db_partition_hashtree.html#terminate/2","title":"plum_db_partition_hashtree.terminate/2","type":"function"},{"doc":"Updates the tree on this node. See also: update/2 .","ref":"plum_db_partition_hashtree.html#update/1","title":"plum_db_partition_hashtree.update/1","type":"function"},{"doc":"Updates the tree on Node . The tree must be locked using one of the lock functions. If the tree is not locked or built the update will not be started and the appropriate atom is returned. Although this function should not be called without a lock, if it is and the tree is being updated by the background tick then ongoing_update is returned. If the tree is built and a lock has been acquired then the update is started and ok is returned. The update is performed asynchronously and does not block the process that manages the tree (e.g. future inserts).","ref":"plum_db_partition_hashtree.html#update/2","title":"plum_db_partition_hashtree.update/2","type":"function"},{"doc":"","ref":"plum_db_partition_hashtree.html#t:plum_db_key/0","title":"plum_db_partition_hashtree.plum_db_key/0","type":"type"},{"doc":"","ref":"plum_db_partition_hashtree.html#t:plum_db_pkey/0","title":"plum_db_partition_hashtree.plum_db_pkey/0","type":"type"},{"doc":"","ref":"plum_db_partition_hashtree.html#t:plum_db_prefix/0","title":"plum_db_partition_hashtree.plum_db_prefix/0","type":"type"},{"doc":"A wrapper for an elevelb instance.","ref":"plum_db_partition_server.html","title":"plum_db_partition_server","type":"module"},{"doc":"","ref":"plum_db_partition_server.html#byte_size/1","title":"plum_db_partition_server.byte_size/1","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#code_change/3","title":"plum_db_partition_server.code_change/3","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#erase/3","title":"plum_db_partition_server.erase/3","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#get/2","title":"plum_db_partition_server.get/2","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#get/3","title":"plum_db_partition_server.get/3","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#get/4","title":"plum_db_partition_server.get/4","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#handle_call/3","title":"plum_db_partition_server.handle_call/3","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#handle_cast/2","title":"plum_db_partition_server.handle_cast/2","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#handle_info/2","title":"plum_db_partition_server.handle_info/2","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#init/1","title":"plum_db_partition_server.init/1","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#is_empty/1","title":"plum_db_partition_server.is_empty/1","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#iterator/2","title":"plum_db_partition_server.iterator/2","type":"function"},{"doc":"If the prefix is ground, it restricts the iteration on keys belonging to that prefix and the storage type of the prefix if known. If prefix is undefined or storage type of is undefined, then it starts with disk and follows with ram. It does not cover ram_disk as all data in ram_disk is in disk but not viceversa.","ref":"plum_db_partition_server.html#iterator/3","title":"plum_db_partition_server.iterator/3","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#iterator_close/2","title":"plum_db_partition_server.iterator_close/2","type":"function"},{"doc":"Iterates over the storage stack in order (disk -&gt; ram_disk -&gt; ram).","ref":"plum_db_partition_server.html#iterator_move/2","title":"plum_db_partition_server.iterator_move/2","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#key_iterator/2","title":"plum_db_partition_server.key_iterator/2","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#key_iterator/3","title":"plum_db_partition_server.key_iterator/3","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#merge/3","title":"plum_db_partition_server.merge/3","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#merge/4","title":"plum_db_partition_server.merge/4","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#put/3","title":"plum_db_partition_server.put/3","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#put/4","title":"plum_db_partition_server.put/4","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#put/5","title":"plum_db_partition_server.put/5","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#start_link/2","title":"plum_db_partition_server.start_link/2","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#take/2","title":"plum_db_partition_server.take/2","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#take/3","title":"plum_db_partition_server.take/3","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#take/4","title":"plum_db_partition_server.take/4","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#terminate/2","title":"plum_db_partition_server.terminate/2","type":"function"},{"doc":"","ref":"plum_db_partition_server.html#t:db_info/0","title":"plum_db_partition_server.db_info/0","type":"type"},{"doc":"","ref":"plum_db_partition_server.html#t:iterator/0","title":"plum_db_partition_server.iterator/0","type":"type"},{"doc":"","ref":"plum_db_partition_server.html#t:iterator_action/0","title":"plum_db_partition_server.iterator_action/0","type":"type"},{"doc":"","ref":"plum_db_partition_server.html#t:iterator_move_result/0","title":"plum_db_partition_server.iterator_move_result/0","type":"type"},{"doc":"","ref":"plum_db_partition_server.html#t:opts/0","title":"plum_db_partition_server.opts/0","type":"type"},{"doc":"","ref":"plum_db_partition_server.html#t:plum_db_key/0","title":"plum_db_partition_server.plum_db_key/0","type":"type"},{"doc":"","ref":"plum_db_partition_server.html#t:plum_db_pkey/0","title":"plum_db_partition_server.plum_db_pkey/0","type":"type"},{"doc":"","ref":"plum_db_partition_server.html#t:plum_db_prefix/0","title":"plum_db_partition_server.plum_db_prefix/0","type":"type"},{"doc":"","ref":"plum_db_partition_server.html#t:plum_db_prefix_pattern/0","title":"plum_db_partition_server.plum_db_prefix_pattern/0","type":"type"},{"doc":"","ref":"plum_db_partition_server.html#t:plum_db_wildcard/0","title":"plum_db_partition_server.plum_db_wildcard/0","type":"type"},{"doc":"","ref":"plum_db_partition_sup.html","title":"plum_db_partition_sup","type":"module"},{"doc":"","ref":"plum_db_partition_sup.html#init/1","title":"plum_db_partition_sup.init/1","type":"function"},{"doc":"","ref":"plum_db_partition_sup.html#start_link/1","title":"plum_db_partition_sup.start_link/1","type":"function"},{"doc":"","ref":"plum_db_partitions_sup.html","title":"plum_db_partitions_sup","type":"module"},{"doc":"","ref":"plum_db_partitions_sup.html#get_db_info/1","title":"plum_db_partitions_sup.get_db_info/1","type":"function"},{"doc":"","ref":"plum_db_partitions_sup.html#init/1","title":"plum_db_partitions_sup.init/1","type":"function"},{"doc":"","ref":"plum_db_partitions_sup.html#set_db_info/2","title":"plum_db_partitions_sup.set_db_info/2","type":"function"},{"doc":"","ref":"plum_db_partitions_sup.html#start_link/0","title":"plum_db_partitions_sup.start_link/0","type":"function"},{"doc":"Based on: github.com/lasp-lang/lasp/...lasp_peer_service.erl","ref":"plum_db_peer_service.html","title":"plum_db_peer_service","type":"behaviour"},{"doc":"Add callback","ref":"plum_db_peer_service.html#add_sup_callback/1","title":"plum_db_peer_service.add_sup_callback/1","type":"function"},{"doc":"Add callback","ref":"plum_db_peer_service.html#cast_message/3","title":"plum_db_peer_service.cast_message/3","type":"function"},{"doc":"Return node connections.","ref":"plum_db_peer_service.html#connections/0","title":"plum_db_peer_service.connections/0","type":"function"},{"doc":"","ref":"plum_db_peer_service.html#c:connections/0","title":"plum_db_peer_service.connections/0","type":"callback"},{"doc":"","ref":"plum_db_peer_service.html#c:decode/0","title":"plum_db_peer_service.decode/0","type":"callback"},{"doc":"","ref":"plum_db_peer_service.html#decode/1","title":"plum_db_peer_service.decode/1","type":"function"},{"doc":"Add callback","ref":"plum_db_peer_service.html#forward_message/3","title":"plum_db_peer_service.forward_message/3","type":"function"},{"doc":"Prepare node to join a cluster.","ref":"plum_db_peer_service.html#join/1","title":"plum_db_peer_service.join/1","type":"function"},{"doc":"","ref":"plum_db_peer_service.html#c:join/1","title":"plum_db_peer_service.join/1","type":"callback"},{"doc":"Convert nodename to atom.","ref":"plum_db_peer_service.html#join/2","title":"plum_db_peer_service.join/2","type":"function"},{"doc":"","ref":"plum_db_peer_service.html#c:join/2","title":"plum_db_peer_service.join/2","type":"callback"},{"doc":"Initiate join. Nodes cannot join themselves.","ref":"plum_db_peer_service.html#join/3","title":"plum_db_peer_service.join/3","type":"function"},{"doc":"","ref":"plum_db_peer_service.html#c:join/3","title":"plum_db_peer_service.join/3","type":"callback"},{"doc":"Leave the cluster.","ref":"plum_db_peer_service.html#leave/0","title":"plum_db_peer_service.leave/0","type":"function"},{"doc":"","ref":"plum_db_peer_service.html#c:leave/0","title":"plum_db_peer_service.leave/0","type":"callback"},{"doc":"Leave the cluster.","ref":"plum_db_peer_service.html#leave/1","title":"plum_db_peer_service.leave/1","type":"function"},{"doc":"","ref":"plum_db_peer_service.html#c:leave/1","title":"plum_db_peer_service.leave/1","type":"callback"},{"doc":"Return manager.","ref":"plum_db_peer_service.html#manager/0","title":"plum_db_peer_service.manager/0","type":"function"},{"doc":"","ref":"plum_db_peer_service.html#c:manager/0","title":"plum_db_peer_service.manager/0","type":"callback"},{"doc":"Return cluster members.","ref":"plum_db_peer_service.html#members/0","title":"plum_db_peer_service.members/0","type":"function"},{"doc":"","ref":"plum_db_peer_service.html#c:members/0","title":"plum_db_peer_service.members/0","type":"callback"},{"doc":"Return myself.","ref":"plum_db_peer_service.html#mynode/0","title":"plum_db_peer_service.mynode/0","type":"function"},{"doc":"","ref":"plum_db_peer_service.html#c:mynode/0","title":"plum_db_peer_service.mynode/0","type":"callback"},{"doc":"Return myself.","ref":"plum_db_peer_service.html#myself/0","title":"plum_db_peer_service.myself/0","type":"function"},{"doc":"","ref":"plum_db_peer_service.html#c:myself/0","title":"plum_db_peer_service.myself/0","type":"callback"},{"doc":"Returns a local peer node i.e. running on the same host, with nodename Name and same metadata as myself/0 . You should only use this function for testing purposes as there is no much sense in running a plum_db cluster on the same host.","ref":"plum_db_peer_service.html#peer/1","title":"plum_db_peer_service.peer/1","type":"function"},{"doc":"Returns a peer with nodename Name and host 'Host' and same metadata as myself/0 .","ref":"plum_db_peer_service.html#peer/2","title":"plum_db_peer_service.peer/2","type":"function"},{"doc":"Return the currently active peer service.","ref":"plum_db_peer_service.html#peer_service/0","title":"plum_db_peer_service.peer_service/0","type":"function"},{"doc":"Stop node.","ref":"plum_db_peer_service.html#stop/0","title":"plum_db_peer_service.stop/0","type":"function"},{"doc":"","ref":"plum_db_peer_service.html#c:stop/0","title":"plum_db_peer_service.stop/0","type":"callback"},{"doc":"Stop node for a given reason.","ref":"plum_db_peer_service.html#stop/1","title":"plum_db_peer_service.stop/1","type":"function"},{"doc":"","ref":"plum_db_peer_service.html#c:stop/1","title":"plum_db_peer_service.stop/1","type":"callback"},{"doc":"Prepare node to join a cluster.","ref":"plum_db_peer_service.html#sync_join/1","title":"plum_db_peer_service.sync_join/1","type":"function"},{"doc":"","ref":"plum_db_peer_service.html#c:sync_join/1","title":"plum_db_peer_service.sync_join/1","type":"callback"},{"doc":"Prepare node to join a cluster.","ref":"plum_db_peer_service.html#sync_join/2","title":"plum_db_peer_service.sync_join/2","type":"function"},{"doc":"","ref":"plum_db_peer_service.html#c:sync_join/2","title":"plum_db_peer_service.sync_join/2","type":"callback"},{"doc":"Prepare node to join a cluster.","ref":"plum_db_peer_service.html#sync_join/3","title":"plum_db_peer_service.sync_join/3","type":"function"},{"doc":"","ref":"plum_db_peer_service.html#c:sync_join/3","title":"plum_db_peer_service.sync_join/3","type":"callback"},{"doc":"Update cluster members.","ref":"plum_db_peer_service.html#update_members/1","title":"plum_db_peer_service.update_members/1","type":"function"},{"doc":"","ref":"plum_db_peer_service.html#t:partisan_peer/0","title":"plum_db_peer_service.partisan_peer/0","type":"type"},{"doc":"Gproc Publish/Subscribe patterns This module implements a few convenient functions for publish/subscribe. Publish/subscribe with Gproc relies entirely on gproc properties and counters. This makes for a very concise implementation, as the monitoring of subscribers and removal of subscriptions comes for free with Gproc. Using this module instead of rolling your own (which is easy enough) brings the benefit of consistency, in tracing and debugging. The implementation can also serve to illustrate how to use gproc properties and counters to good effect. -type scope() = l | g. -type event() = any(). -type msg() = any(). -type status() = 1 | 0.","ref":"plum_db_pubsub.html","title":"plum_db_pubsub","type":"module"},{"doc":"Change the condition specification of an existing subscription. This function atomically changes the condition spec of an existing subscription (see subscribe_cond/3 ). An exception is raised if the subscription doesn't already exist. Note that this function can also be used to change a conditional subscription to an unconditional one (by setting Spec = undefined ), or a 'normal' subscription to a conditional one.","ref":"plum_db_pubsub.html#change_cond/3","title":"plum_db_pubsub.change_cond/3","type":"function"},{"doc":"Creates a single-shot subscription entry for Event Single-shot subscriptions behave similarly to the {active,once} property of sockets. Once a message has been published, the subscription is disabled, and no more messages will be delivered to the subscriber unless the subscription is re-enabled using enable_single/2 . The function creates a gproc counter entry, {c,Scope,{gproc_ps_event,Event}} , which will have either of the values 0 (disabled) or 1 (enabled). Initially, the value is 1 , meaning the subscription is enabled. Counters are used in this case, since they can be atomically updated by both the subscriber (owner) and publisher. The publisher sets the counter value to 0 as soon as it has delivered a message.","ref":"plum_db_pubsub.html#create_single/2","title":"plum_db_pubsub.create_single/2","type":"function"},{"doc":"Deletes the single-shot subscription for Event This function deletes the counter entry representing the single-shot description. An exception will be raised if there is no such subscription.","ref":"plum_db_pubsub.html#delete_single/2","title":"plum_db_pubsub.delete_single/2","type":"function"},{"doc":"Disables the single-shot subscription for Event This function changes the value of the corresponding gproc counter to 0 (disabled). The subscription remains (e.g. for debugging purposes), but with a 'disabled' status. This function is insensitive to concurrency, using 'wrapping' ets counter update ops. This guarantees that the counter will have either the value 1 or 0, depending on which update happened last. The return value indicates the previous status.","ref":"plum_db_pubsub.html#disable_single/2","title":"plum_db_pubsub.disable_single/2","type":"function"},{"doc":"Enables the single-shot subscription for Event This function changes the value of the corresponding gproc counter to 1 (enabled). After enabling, the subscriber will receive the next message published for Event , after which the subscription is automatically disabled. This function is insensitive to concurrency, using 'wrapping' ets counter update ops. This guarantees that the counter will have either the value 1 or 0, depending on which update happened last. The return value indicates the previous status.","ref":"plum_db_pubsub.html#enable_single/2","title":"plum_db_pubsub.enable_single/2","type":"function"},{"doc":"Lists all single-shot subscribers of Event, together with their status","ref":"plum_db_pubsub.html#list_singles/2","title":"plum_db_pubsub.list_singles/2","type":"function"},{"doc":"List the pids of all processes subscribing to Event This function uses gproc:select/2 to find all properties indicating a subscription.","ref":"plum_db_pubsub.html#list_subs/2","title":"plum_db_pubsub.list_subs/2","type":"function"},{"doc":"Create/enable a single subscription for event; notify at once if F() -&gt; true This function is a convenience function, wrapping a single-shot pub/sub around a user-provided boolean test. Msg should be what the publisher will send later, if the immediate test returns false .","ref":"plum_db_pubsub.html#notify_single_if_true/4","title":"plum_db_pubsub.notify_single_if_true/4","type":"function"},{"doc":"Publish the message Msg to all subscribers of Event The message delivered to each subscriber will be of the form: {gproc_ps_event, Event, Msg} The function uses gproc:send/2 to send a message to all processes which have a property {p,Scope,{gproc_ps_event,Event}} .","ref":"plum_db_pubsub.html#publish/3","title":"plum_db_pubsub.publish/3","type":"function"},{"doc":"Publishes the message Msg to conditional subscribers of Event The message will be delivered to each subscriber provided their respective condition tests succeed. See also: subscribe_cond/3 .","ref":"plum_db_pubsub.html#publish_cond/3","title":"plum_db_pubsub.publish_cond/3","type":"function"},{"doc":"Subscribe to events of type Event Any messages published with gproc_ps:publish(Scope, Event, Msg) will be delivered to the current process, along with all other subscribers. This function creates a property, {p,Scope,{gproc_ps_event,Event}} , which can be searched and displayed for debugging purposes. Note that, as with gproc:reg/1 , this function will raise an exception if you try to subscribe to the same event twice from the same process.","ref":"plum_db_pubsub.html#subscribe/2","title":"plum_db_pubsub.subscribe/2","type":"function"},{"doc":"Subscribe conditionally to events of type Event This function is similar to subscribe/2 , but adds a condition in the form of a match specification. The condition is tested by the publish_cond/3 function and a message is delivered only if the condition is true. Specifically, the test is: ets:match_spec_run([Msg], ets:match_spec_compile(Cond)) == [true] In other words, if the match_spec returns true for a message, that message is sent to the subscriber. For any other result from the match_spec, the message is not sent. Cond == undefined means that all messages will be delivered (that is, publish_cond/3 will treat 'normal' subscribers just like publish/3 does, except that publish/3 strictly speaking ignores the Value part of the property completely, whereas publish_cond/3 expects it to be either undefined or a valid match spec). This means that Cond=undefined and Cond=[{'_',[],[true]}] are equivalent. Note that, as with gproc:reg/1 , this function will raise an exception if you try to subscribe to the same event twice from the same process.","ref":"plum_db_pubsub.html#subscribe_cond/3","title":"plum_db_pubsub.subscribe_cond/3","type":"function"},{"doc":"Publish Msg to all single-shot subscribers of Event The subscriber status of each active subscriber is changed to 0 (disabled) before delivering the message. This reduces the risk that two different processes will be able to both deliver a message before disabling the subscribers. This could happen if the context switch happens just after the select operation (finding the active subscribers) and before the process is able to update the counters. In this case, it is possible that more than one can be delivered. The way to prevent this from happening is to ensure that only one process publishes for Event .","ref":"plum_db_pubsub.html#tell_singles/3","title":"plum_db_pubsub.tell_singles/3","type":"function"},{"doc":"Remove subscribtion created using subscribe(Scope, Event) This removes the property created through subscribe/2 .","ref":"plum_db_pubsub.html#unsubscribe/2","title":"plum_db_pubsub.unsubscribe/2","type":"function"},{"doc":"","ref":"plum_db_pubsub.html#t:event/0","title":"plum_db_pubsub.event/0","type":"type"},{"doc":"","ref":"plum_db_pubsub.html#t:msg/0","title":"plum_db_pubsub.msg/0","type":"type"},{"doc":"","ref":"plum_db_pubsub.html#t:scope/0","title":"plum_db_pubsub.scope/0","type":"type"},{"doc":"","ref":"plum_db_pubsub.html#t:status/0","title":"plum_db_pubsub.status/0","type":"type"},{"doc":"A transient worker that is used to listen to certain plum_db events and allow watchers to wait (blocking the caller) for certain conditions. This is used by plum_db_app during the startup process to wait for the following conditions: * Partition initisalisation – the worker subscribes to plum_db notifications and keeps track of each partition initialisation until they are all initialised (or failed to initilised) and replies to all watchers with a ok or {error, FailedPartitions} , where FailedPartitions is a map() which keys are the partition number and the value is the reason for the failure. * Partition hashtree build – the worker subscribes to plum_db notifications and keeps track of each partition hashtree until they are all built (or failed to build) and replies to all watchers with a ok or {error, FailedHashtrees} , where FailedHashtrees is a map() which keys are the partition number and the value is the reason for the failure. A watcher is any process which calls the functions wait_for_partitions/0,1 and/or wait_for_hashtrees/0,1. Both functions will block the caller until the above conditions are met.","ref":"plum_db_startup_coordinator.html","title":"plum_db_startup_coordinator","type":"module"},{"doc":"Start plumtree_partitions_coordinator and link to calling process.","ref":"plum_db_startup_coordinator.html#start_link/0","title":"plum_db_startup_coordinator.start_link/0","type":"function"},{"doc":"","ref":"plum_db_startup_coordinator.html#stop/0","title":"plum_db_startup_coordinator.stop/0","type":"function"},{"doc":"Blocks the caller until all hastrees are built. This is equivalent to calling wait_for_hashtrees(infinity) .","ref":"plum_db_startup_coordinator.html#wait_for_hashtrees/0","title":"plum_db_startup_coordinator.wait_for_hashtrees/0","type":"function"},{"doc":"Blocks the caller until all hastrees are built or the timeout TImeout is reached.","ref":"plum_db_startup_coordinator.html#wait_for_hashtrees/1","title":"plum_db_startup_coordinator.wait_for_hashtrees/1","type":"function"},{"doc":"Blocks the caller until all partitions are initialised. This is equivalent to calling wait_for_partitions(infinity) .","ref":"plum_db_startup_coordinator.html#wait_for_partitions/0","title":"plum_db_startup_coordinator.wait_for_partitions/0","type":"function"},{"doc":"Blocks the caller until all partitions are initialised or the timeout TImeout is reached.","ref":"plum_db_startup_coordinator.html#wait_for_partitions/1","title":"plum_db_startup_coordinator.wait_for_partitions/1","type":"function"},{"doc":"","ref":"plum_db_startup_coordinator.html#t:state/0","title":"plum_db_startup_coordinator.state/0","type":"type"},{"doc":"","ref":"plum_db_sup.html","title":"plum_db_sup","type":"module"},{"doc":"","ref":"plum_db_sup.html#init/1","title":"plum_db_sup.init/1","type":"function"},{"doc":"Initialises plum_db configuration and starts the supervisor","ref":"plum_db_sup.html#start_link/0","title":"plum_db_sup.start_link/0","type":"function"},{"doc":"","ref":"plum_db_table_owner.html","title":"plum_db_table_owner","type":"module"},{"doc":"Creates a new ets table, sets itself as heir.","ref":"plum_db_table_owner.html#add/2","title":"plum_db_table_owner.add/2","type":"function"},{"doc":"Creates a new ets table, sets itself as heir and gives it away to Requester","ref":"plum_db_table_owner.html#add_and_claim/2","title":"plum_db_table_owner.add_and_claim/2","type":"function"},{"doc":"If the table exists, it gives it away to Requester. Otherwise, creates a new ets table, sets itself as heir and gives it away to Requester.","ref":"plum_db_table_owner.html#add_or_claim/2","title":"plum_db_table_owner.add_or_claim/2","type":"function"},{"doc":"Used by the table owner to delegate the ownership to the calling process. The process must be local and not already the owner of the table.","ref":"plum_db_table_owner.html#claim/1","title":"plum_db_table_owner.claim/1","type":"function"},{"doc":"","ref":"plum_db_table_owner.html#code_change/3","title":"plum_db_table_owner.code_change/3","type":"function"},{"doc":"Deletes the ets table with name Name iff the caller is the owner.","ref":"plum_db_table_owner.html#delete/1","title":"plum_db_table_owner.delete/1","type":"function"},{"doc":"","ref":"plum_db_table_owner.html#exists/1","title":"plum_db_table_owner.exists/1","type":"function"},{"doc":"Used by the table owner to delegate the ownership to another process. NewOwner must be alive, local and not already the owner of the table.","ref":"plum_db_table_owner.html#give_away/2","title":"plum_db_table_owner.give_away/2","type":"function"},{"doc":"","ref":"plum_db_table_owner.html#handle_call/3","title":"plum_db_table_owner.handle_call/3","type":"function"},{"doc":"","ref":"plum_db_table_owner.html#handle_cast/2","title":"plum_db_table_owner.handle_cast/2","type":"function"},{"doc":"","ref":"plum_db_table_owner.html#handle_info/2","title":"plum_db_table_owner.handle_info/2","type":"function"},{"doc":"","ref":"plum_db_table_owner.html#init/1","title":"plum_db_table_owner.init/1","type":"function"},{"doc":"","ref":"plum_db_table_owner.html#lookup/1","title":"plum_db_table_owner.lookup/1","type":"function"},{"doc":"","ref":"plum_db_table_owner.html#start_link/0","title":"plum_db_table_owner.start_link/0","type":"function"},{"doc":"","ref":"plum_db_table_owner.html#terminate/2","title":"plum_db_table_owner.terminate/2","type":"function"},{"doc":"Copyright (c) 2018, Leapsight Copyright (c) 2016, Christopher Meiklejohn Copyright (c) 2015, Basho Technologies, Inc Copyright (c) 2015, Helium Systems, Inc All rights reserved. Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. &quot;License&quot; shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. &quot;Licensor&quot; shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. &quot;Legal Entity&quot; shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, &quot;control&quot; means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. &quot;You&quot; (or &quot;Your&quot;) shall mean an individual or Legal Entity exercising permissions granted by this License. &quot;Source&quot; form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. &quot;Object&quot; form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. &quot;Work&quot; shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). &quot;Derivative Works&quot; shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. &quot;Contribution&quot; shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, &quot;submitted&quot; means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as &quot;Not a Contribution.&quot; &quot;Contributor&quot; shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a &quot;NOTICE&quot; text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS","ref":"license.html","title":"LICENSE","type":"extras"},{"doc":"[[ TOC ]] PlumDB is a globally replicated database using eventual consistency. It uses Epidemic Broadcast Trees and lasp-lang’s Partisan , an alternative runtime system for improved scalability and reduced latency in distributed Erlang applications. It is an offspring of Helium's Plumtree – a descendant of Riak Core 's Metadata Store – and Partisan. The original Plumtree project was the result of extracting the Metadata Store from Riak Core and replacing the cluster membership state by an ORSWOT CRDT. PlumDB builds on top of Plumtree but changes its architecture offering additional features. Feature PlumDB Plumtree Cluster membership state Partisan's membership state which uses an AWSET ORSWOT (riak_dt) Data model Riak Core Metadata (dvvset) Riak Core Metadata (dvvset) Persistence leveldb. A key is sharded across N instances of a store. Stores can be in-memory ( ets ), on disk (Basho's Eleveldb i.e. Leveldb) or both. N is configurable at deployment time. Each prefix has its own ets and dets table. API A simplification of the Riak Core Metadata API. A single function to iterate over the whole database i.e. across one or all shards and across a single or many prefixes. Riak Core Metadata API ( plumtree_metadata_manager ) is used to iterate over prefixes whereas plumtree_metadata is used to iterate over keys within each prefix. The API is confusing and is the result of having a store (ets + dets) per prefix. Active anti-entropy Based on Riak Core Metadata AAE, uses a separate instance of leveldb to store a merkle tree on disk. Updated to use the new API and gen_statem Based on Riak Core Metadata AAE, uses a separate instance of leveldb to store a merkle tree on disk. Pubsub Based on a combination of gen_event and gproc , allowing to register a Callback module or function to be executed when an event is generated. gproc dependency allows to pattern match events using a match spec Based on gen_event, allowing to register a Callback module or function to be executed when an event is generated","ref":"readme.html","title":"PlumDB","type":"extras"},{"doc":"You will use PlumDB as a dependency in your Erlang application. Configuration PlumDB is configured using the standard Erlang sys.config. The following is an example configuration: { plum_db , [ { aae_enabled , true } , { store_open_retries_delay , 2000 } , { store_open_retry_Limit , 30 } , { data_exchange_timeout , 60000 } , { hashtree_timer , 10000 } , { data_dir , &quot;data&quot; } , { partitions , 8 } , { prefixes , [ { foo , ram } , { bar , ram_disk } , { &lt;&lt; &quot;baz&quot; &gt;&gt; , disk } ] } ] } partitions (integer) – the number of shards. prefixes – a list of {Prefix, prefix_type()} Prefix is a user defined atom or binary prefix_type() is one of ram , ram_disk and disk . aae_enabled (boolean) – whether the Active Anti-Entropy mechanism is enabled. store_open_retries_delay (milliseconds) – controls thre underlying disk store (leveldb) delay between open retries. store_open_retry_Limit (integer) – controls thre underlying disk store (leveldb) open retry limit data_exchange_timeout (milliseconds) – the timeout for the AAE workers hashtree_timer (seconds) – At the moment additional configuration is required for Partisan and Plumtree dependencies: { partisan , [ { peer_port , 18086 } , % port for inter-node communication { parallelism , 4 } % number of tcp connections ] } { plumtree , [ { broadcast_exchange_timer , 60000 } % Perform AAE exchange every 1 min. ] }","ref":"readme.html#installation","title":"PlumDB - Installation","type":"extras"},{"doc":"Learn more by reading the source code Documentation .","ref":"readme.html#usage","title":"PlumDB - Usage","type":"extras"},{"doc":"We have three rebar3 release profiles that you can use for testing PlumDB itself. Running a 3-node cluster To run a three node cluster do the following in three separate shells. In shell #1: $ rebar3 as dev1 run In shell #2: $ rebar3 as dev2 run In shell #3: $ rebar3 as dev3 run Make node 2 and 3 join node 1 In node #2: &gt; Peer = plum_db_peer_service : peer ( &#39;plum_db1@127.0.0.1&#39; , { { 127 , 0 , 0 , 1 } , 18086 } ) . &gt; plum_db_peer_service : join ( Peer ) . In node #3: &gt; Peer = plum_db_peer_service : peer ( &#39;plum_db1@127.0.0.1&#39; , { { 127 , 0 , 0 , 1 } , 18086 } ) . &gt; plum_db_peer_service : join ( Peer ) . Check that the other two nodes are visible in each node In node #1: &gt; plum_db_peer_service : members ( ) . { ok , [ &#39;plum_db3@127.0.0.1&#39; , &#39;plum_db2@127.0.0.1&#39; ] } In node #2: &gt; plum_db_peer_service : members ( ) . { ok , [ &#39;plum_db3@127.0.0.1&#39; , &#39;plum_db1@127.0.0.1&#39; ] } In node #3: &gt; plum_db_peer_service : members ( ) . { ok , [ &#39;plum_db2@127.0.0.1&#39; , &#39;plum_db1@127.0.0.1&#39; ] } In node #1: &gt; [ plum_db : put ( { foo , a } , x , 1 ) . ok In node #2: &gt; plum_db : put ( { foo , a } , y , 2 ) . ok In node #3: &gt; plum_db : put ( { foo , a } , z , 3 ) . ok Do the following on each node to check they now all have the three elements: &gt; plum_db : fold ( fun ( Tuple , Acc ) -&gt; [ Tuple | Acc ] end , [ ] , { &#39;_&#39; , &#39;_&#39; } ) . [ { x , 1 } , { y , 2 } , { z , 3 } ] We are folding over the whole database (all shards) using the full prefix wildcard {'_', '_'} . The following are examples of prefix wildcards: {'_', '_'} - matches all full prefixes {foo, '_'} - matches all subprefixes of Prefix foo {foo, x} - matches the subprefix x of prefix foo Notice that the pattern {'_', bar} is NOT allowed.","ref":"readme.html#standalone-testing","title":"PlumDB - Standalone testing","type":"extras"}]